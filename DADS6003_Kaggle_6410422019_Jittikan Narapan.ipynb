{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9662a65f",
   "metadata": {},
   "source": [
    "# Jittikan Narapan ID: 6410422019"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b56be5c",
   "metadata": {},
   "source": [
    "## Import Libraries and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08d6be47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5edf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "plt.rc(\"font\", size=14) #dictionary objects\n",
    "pylab.rcParams['figure.figsize'] = 16,16\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\") #white background style for seaborn plots\n",
    "sns.set(style=\"whitegrid\", color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1044b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from mlxtend.preprocessing import minmax_scaling\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import learning_curve, validation_curve\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, precision_score, recall_score \n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c16ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Datasets\n",
    "df_train = pd.read_csv('/kaggle/input/dads6003-in-class-competition/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/dads6003-in-class-competition/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555fa77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preview Training Data\n",
    "print(df_train.info())\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6081f29a",
   "metadata": {},
   "source": [
    "There are null values in feature Xs (Except for x9) in Training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b9af01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preview Testing Data\n",
    "print(df_test.info())\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7192631",
   "metadata": {},
   "source": [
    "There are no null values in Testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24490737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the number of samples\n",
    "print('The number of samples in the training dataset is {}.'.format(df_train.shape[0]))\n",
    "print('The number of samples in the test dataset is {}.'.format(df_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c79b64",
   "metadata": {},
   "source": [
    "## Create Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f9cb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing Values Data Analysis\n",
    "\n",
    "def missingdata(data):\n",
    "    total = data.isnull().sum().sort_values(ascending = False)\n",
    "    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n",
    "    ms=pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    ms= ms[ms[\"Percent\"] > 0]\n",
    "    f,ax =plt.subplots(figsize=(8,6))\n",
    "    plt.xticks(rotation='90')\n",
    "    fig=sns.barplot(ms.index, ms[\"Percent\"],color=\"purple\",alpha=0.8)\n",
    "    plt.xlabel('Features', fontsize=15)\n",
    "    plt.ylabel('%Missing values', fontsize=15)\n",
    "    plt.title('Percentage of Null values by features', fontsize=15)\n",
    "    return ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d8f0f9",
   "metadata": {},
   "source": [
    "## 1. Cleansing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db5e63e",
   "metadata": {},
   "source": [
    "### 1.1 Missing/Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfb2522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check missing values\n",
    "missingdata(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49131de0",
   "metadata": {},
   "source": [
    "As mentioned earlier that only x9 contains no null value while there are less than 2% of missing values in other features. Therefore, We can use all 20 features in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf6c463",
   "metadata": {},
   "source": [
    "### 1.2 Data Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fbdc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Description\n",
    "df_train.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e4a8a3",
   "metadata": {},
   "source": [
    "We can see that the ranges of data in each feature are much different --> Need scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1556f",
   "metadata": {},
   "source": [
    "### 1.3 Check Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6637083",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(ncols=4,nrows=5, figsize=(25,25))\n",
    "data=df_train\n",
    "\n",
    "sns.boxplot(x='y', y='x1', data=data, palette='autumn', ax=axes[0,0])\n",
    "sns.boxplot(x='y', y='x2', data=data, palette='autumn', ax=axes[0,1])\n",
    "sns.boxplot(x='y', y='x3', data=data, palette='autumn', ax=axes[0,2])\n",
    "sns.boxplot(x='y', y='x4', data=data, palette='autumn', ax=axes[0,3])\n",
    "\n",
    "sns.boxplot(x='y', y='x5', data=data, palette='autumn', ax=axes[1,0])\n",
    "sns.boxplot(x='y', y='x6', data=data, palette='autumn', ax=axes[1,1])\n",
    "sns.boxplot(x='y', y='x7', data=data, palette='autumn', ax=axes[1,2])\n",
    "sns.boxplot(x='y', y='x8', data=data, palette='autumn', ax=axes[1,3])\n",
    "\n",
    "sns.boxplot(x='y', y='x9', data=data, palette='autumn', ax=axes[2,0])\n",
    "sns.boxplot(x='y', y='x10', data=data, palette='autumn', ax=axes[2,1])\n",
    "sns.boxplot(x='y', y='x11', data=data, palette='autumn', ax=axes[2,2])\n",
    "sns.boxplot(x='y', y='x12', data=data, palette='autumn', ax=axes[2,3])\n",
    "\n",
    "sns.boxplot(x='y', y='x13', data=data, palette='autumn', ax=axes[3,0])\n",
    "sns.boxplot(x='y', y='x14', data=data, palette='autumn', ax=axes[3,1])\n",
    "sns.boxplot(x='y', y='x15', data=data, palette='autumn', ax=axes[3,2])\n",
    "sns.boxplot(x='y', y='x16', data=data, palette='autumn', ax=axes[3,3])\n",
    "\n",
    "sns.boxplot(x='y', y='x17', data=data, palette='autumn', ax=axes[4,0])\n",
    "sns.boxplot(x='y', y='x18', data=data, palette='autumn', ax=axes[4,1])\n",
    "sns.boxplot(x='y', y='x19', data=data, palette='autumn', ax=axes[4,2])\n",
    "sns.boxplot(x='y', y='x20', data=data, palette='autumn', ax=axes[4,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7568cfe2",
   "metadata": {},
   "source": [
    "From the box plots, we can see that there are some outliers in each features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb326d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Extreme Outliers\n",
    "\n",
    "Q1 = df_train.quantile(0.25)\n",
    "Q3 = df_train.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "df1 = df_train[~((df_train < (Q1 - 1.5 * IQR)) |(df_train > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "print(df1.shape)\n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8a022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replot \n",
    "f, axes = plt.subplots(ncols=4,nrows=5, figsize=(25,25))\n",
    "data=df1\n",
    "\n",
    "sns.boxplot(x='y', y='x1', data=data, palette='magma', ax=axes[0,0])\n",
    "sns.boxplot(x='y', y='x2', data=data, palette='magma', ax=axes[0,1])\n",
    "sns.boxplot(x='y', y='x3', data=data, palette='magma', ax=axes[0,2])\n",
    "sns.boxplot(x='y', y='x4', data=data, palette='magma', ax=axes[0,3])\n",
    "\n",
    "sns.boxplot(x='y', y='x5', data=data, palette='magma', ax=axes[1,0])\n",
    "sns.boxplot(x='y', y='x6', data=data, palette='magma', ax=axes[1,1])\n",
    "sns.boxplot(x='y', y='x7', data=data, palette='magma', ax=axes[1,2])\n",
    "sns.boxplot(x='y', y='x8', data=data, palette='magma', ax=axes[1,3])\n",
    "\n",
    "sns.boxplot(x='y', y='x9', data=data, palette='magma', ax=axes[2,0])\n",
    "sns.boxplot(x='y', y='x10', data=data, palette='magma', ax=axes[2,1])\n",
    "sns.boxplot(x='y', y='x11', data=data, palette='magma', ax=axes[2,2])\n",
    "sns.boxplot(x='y', y='x12', data=data, palette='magma', ax=axes[2,3])\n",
    "\n",
    "sns.boxplot(x='y', y='x13', data=data, palette='magma', ax=axes[3,0])\n",
    "sns.boxplot(x='y', y='x14', data=data, palette='magma', ax=axes[3,1])\n",
    "sns.boxplot(x='y', y='x15', data=data, palette='magma', ax=axes[3,2])\n",
    "sns.boxplot(x='y', y='x16', data=data, palette='magma', ax=axes[3,3])\n",
    "\n",
    "sns.boxplot(x='y', y='x17', data=data, palette='magma', ax=axes[4,0])\n",
    "sns.boxplot(x='y', y='x18', data=data, palette='magma', ax=axes[4,1])\n",
    "sns.boxplot(x='y', y='x19', data=data, palette='magma', ax=axes[4,2])\n",
    "sns.boxplot(x='y', y='x20', data=data, palette='magma', ax=axes[4,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca1f0f2",
   "metadata": {},
   "source": [
    "From box plots above, extreme outliers are removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd69680",
   "metadata": {},
   "source": [
    "### 1.4 Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9b4294",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if in each binary results in 'y', means of each feature are different or not.\n",
    "df=df1.groupby(['y']).mean()\n",
    "print(df)\n",
    "print(df.pct_change()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0356a6e",
   "metadata": {},
   "source": [
    "Difference of means are significantly noticed in features x2, x3, x9, and x15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c90f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check distribution of features data grouped by y values\n",
    "ax = df1.groupby(['y']).hist(bins=10, figsize=(18,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42de5cb5",
   "metadata": {},
   "source": [
    "* y = 0: Means of each feature show normal distributions.\n",
    "* y = 1: Most of the features show normal distributions but some features show skewed/slightly skewed distributions (x1,x2, x3, x14, and x15).\n",
    "* Therefore, We will replace Null values in each features with Median values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873564f8",
   "metadata": {},
   "source": [
    "### 1.5 Fill Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f814429",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill missing values with mean grouped by y\n",
    "\n",
    "for i in df1:\n",
    "    df1[i] = df1[i].fillna(df1.groupby('y')[i].transform('median'))\n",
    "\n",
    "#Check if we fill all the Null values\n",
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bd27d2",
   "metadata": {},
   "source": [
    "Now, we have data ready for further EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45217728",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (describe insight and visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3200fa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Percentage of y results in dataset - Check if data is imbalanced or not\n",
    "df1['y'].value_counts()\n",
    "ax = sns.countplot(x='y',data=df1, palette='hls')\n",
    "ax.bar_label(ax.containers[0])\n",
    "print('y: 0', round(df1['y'].value_counts()[0]/len(df1) * 100,2), '% of the dataset')\n",
    "print('y: 1', round(df1['y'].value_counts()[1]/len(df1) * 100,2), '% of the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00410178",
   "metadata": {},
   "source": [
    "This dataset is considered as imbalanced with approximated ratio of y=1 to y=0 of 1/4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5e7d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Violin plot to see each feature distributions to y value\n",
    "df_train_scale = minmax_scaling(df_train, df_train.columns, 1)\n",
    "df2 = pd.melt(df_train_scale,id_vars='y',var_name = 'features',value_name='value')\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.violinplot(x='features',y='value',hue='y',data=df2,split=True,inner='quart')\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef4233f",
   "metadata": {},
   "source": [
    "Features x2,x3,x9, and x15 show significant differences in means grouped by y-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fb584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify Correlated Variables - Correlation Map\n",
    "\n",
    "f,axes = plt.subplots(ncols=2,figsize=(24,10))\n",
    "\n",
    "sns.heatmap(df_train.corr(), annot=True, linewidths=.5,fmt='.1f',ax=axes[0])\n",
    "axes[0].set_title('Training Data Features Correlation with Y')\n",
    "sns.heatmap(df_test.corr(), annot=True, linewidths=.5,fmt='.1f',ax=axes[1])\n",
    "axes[1].set_title('Testing Data Features Correlation')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f6aa45",
   "metadata": {},
   "source": [
    "* Training dataset shows that y value correlates to main 4 features: x2,x3,x9, and x15 with approximated correlation efficient -0.4 to -0.5.\n",
    "* These 4 features also show correlations with x6,x7,x8,x10,x12, and x14 --- Similar correlations among features to Testing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f81f5b0",
   "metadata": {},
   "source": [
    "## 3. Training Data - Cross Validate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63bbefb",
   "metadata": {},
   "source": [
    "### 3.1 Split Data and Scaling Data\n",
    "* Using StratifiedKFold for splitting data as we've seen imbalanced data in our training dataset to ensure the similar proportion are splitted into the training and validating datasets\n",
    "* Scaling data after splitting as we've seen the different range of data in each feature Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1ef6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split and Scaling Training and Testing (Validating) Data in Training File using train 80% and test 20%\n",
    "\n",
    "df1_x = ['x1','x2','x3','x4','x5','x6','x7','x8','x9','x10','x11','x12','x13','x14','x15','x16','x17','x18','x19','x20'] \n",
    "X = df1[df1_x]\n",
    "y = df1['y']\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42,shuffle=True)\n",
    "\n",
    "#train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.20,random_state=0) (Hold-out Method)\n",
    "\n",
    "for train_index, test_index in skf.split(X,y):\n",
    "    train_x, test_x = X.iloc[train_index], X.iloc[test_index]\n",
    "    train_y, test_y = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "#Scale data & Define Scaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_x)\n",
    "\n",
    "train_x_norm = pd.DataFrame(scaler.transform(train_x))        #Scaling training dataset\n",
    "test_x_norm = pd.DataFrame(scaler.transform(test_x))          #Scaling validating dataset\n",
    "df_test_norm = pd.DataFrame(scaler.transform(df_test))        #Scaling testing dataset\n",
    "\n",
    "train_x_norm.columns = df1_x\n",
    "test_x_norm.columns = df1_x\n",
    "df_test_norm.columns = df1_x\n",
    "\n",
    "df_train_norm = pd.concat((train_x_norm, train_y), 1)\n",
    "df_val_norm = pd.concat((test_x_norm,test_y),1)\n",
    "\n",
    "print('df1 Shape: {}'.format(df1.shape))\n",
    "print('Train Shape: {}'.format(train_x.shape))\n",
    "print('Validation(Test) Shape: {}'.format(test_x.shape))\n",
    "\n",
    "print(train_y.value_counts())\n",
    "print('y: 0', round(train_y.value_counts()[0]/len(train_y) * 100,2), '% of the dataset')\n",
    "print('y: 1', round(train_y.value_counts()[1]/len(train_y) * 100,2), '% of the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6413e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Map\n",
    "\n",
    "f,axes = plt.subplots(ncols=3,figsize=(36,12))\n",
    "\n",
    "sns.heatmap(train_x_norm.corr(), annot=True, linewidths=.5,fmt='.1f',ax=axes[0])\n",
    "axes[0].set_title('Training Data Correlation')\n",
    "sns.heatmap(test_x_norm.corr(), annot=True, linewidths=.5,fmt='.1f',ax=axes[1])\n",
    "axes[1].set_title('Validating Data Correlation')\n",
    "sns.heatmap(df_test_norm.corr(), annot=True, linewidths=.5,fmt='.1f',ax=axes[2])\n",
    "axes[2].set_title('Testing Data Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847f83d9",
   "metadata": {},
   "source": [
    "After Scaling datasets, training, validating, and testing datasets show the same correlation maps with previous relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76843a68",
   "metadata": {},
   "source": [
    "### 3.2 Build ML Models: Using all 20 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a290428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spot Check Algorithms\n",
    "models = []\n",
    "models.append(('LogisticRegression', LogisticRegression()))\n",
    "models.append(('KNearestNeighbors', KNeighborsClassifier()))\n",
    "models.append(('DecisionTree', DecisionTreeClassifier()))\n",
    "models.append(('SupportVectorMachines', SVC()))\n",
    "models.append(('RandomForest', RandomForestClassifier()))\n",
    "#Evaluate training score for each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = StratifiedKFold(n_splits=5, random_state=42,shuffle=True)\n",
    "    cv_results = cross_val_score(model, train_x_norm, train_y, cv=kfold, scoring='f1_weighted')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88ab15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare Algorithms\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.boxplot(results, labels=names)\n",
    "plt.title('Algorithms Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c31ebd",
   "metadata": {},
   "source": [
    "Preliminary, we can choose top two algorithms to model the datasets: SupportVectorMachines and RandomForest\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0440205a",
   "metadata": {},
   "source": [
    "### 3.3 Feature Selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6c7049",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features Selection - Recursive Feature Elimination with Cross Validation and Random Forest\n",
    "\n",
    "clf_rf = RandomForestClassifier()\n",
    "rfecv = RFECV(estimator=clf_rf, step=1, cv=10, scoring='f1_weighted') #10-fold cross-validation\n",
    "rfecv = rfecv.fit(train_x_norm,train_y)\n",
    "\n",
    "print('Optimal number of features: ', rfecv.n_features_)\n",
    "print('Best Features: ',train_x_norm.columns[rfecv.support_])\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d30e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selected Features\n",
    "selfeat = ['x2', 'x3', 'x4','x6', 'x7', 'x8', 'x9', 'x10', 'x12', 'x14', 'x15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd337e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spot Check Algorithms for Selected Features\n",
    "models = []\n",
    "models.append(('LogisticRegression', LogisticRegression()))\n",
    "models.append(('KNearestNeighbors', KNeighborsClassifier()))\n",
    "models.append(('DecisionTree', DecisionTreeClassifier()))\n",
    "models.append(('SupportVectorMachines', SVC()))\n",
    "models.append(('RandomForest', RandomForestClassifier()))\n",
    "#Evaluate training score for each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = StratifiedKFold(n_splits=5, random_state=42,shuffle=True)\n",
    "    cv_results = cross_val_score(model, train_x_norm[selfeat], train_y, cv=kfold, scoring='f1_weighted')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb88c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare Algorithms for Selected Features\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.boxplot(results, labels=names)\n",
    "plt.title('Algorithms Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa44d4c5",
   "metadata": {},
   "source": [
    "After features selection, we can choose top three algorithms to model the datasets: SupportVectorMachines, KNearestNeighbors, and RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b59e286",
   "metadata": {},
   "source": [
    "## 4. Testing Data - with Validating dataset\n",
    "* Test Validating datasets with top three algorithms: SupportVectorMachines, KNearestNeighbors, and RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf44499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SupportVectorMachines\n",
    "svc = SVC(gamma='auto')\n",
    "svc.fit(train_x_norm[selfeat], train_y)\n",
    "y_pred_svc = svc.predict(test_x_norm[selfeat])\n",
    "svc_cf = confusion_matrix(test_y,y_pred_svc)\n",
    "print('SupportVectorMachines:')\n",
    "print(classification_report(test_y, y_pred_svc))\n",
    "f1svc = '{0:.4g}'.format(f1_score(test_y, y_pred_svc, average='weighted'))\n",
    "accsvc = '{0:.4g}'.format(accuracy_score(test_y, y_pred_svc))\n",
    "print('SVC F1 Score: ',f1svc)\n",
    "print('SVC Accuracy Score: ',accsvc)\n",
    "\n",
    "#KNearestNeighbors\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(train_x_norm[selfeat], train_y)\n",
    "y_pred_knn = knn.predict(test_x_norm[selfeat])\n",
    "knn_cf = confusion_matrix(test_y,y_pred_knn)\n",
    "print('KNearestNeighbors:')\n",
    "print(classification_report(test_y, y_pred_knn))\n",
    "f1knn = '{0:.4g}'.format(f1_score(test_y, y_pred_knn, average='weighted'))\n",
    "accknn = '{0:.4g}'.format(accuracy_score(test_y, y_pred_knn))\n",
    "print('KNearestNeighbors F1 Score: ',f1knn)\n",
    "print('KNearestNeighbors Accuracy Score: ',accknn)\n",
    "\n",
    "#RandomForest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(train_x_norm[selfeat], train_y)\n",
    "y_pred_rf = rf.predict(test_x_norm[selfeat])\n",
    "rf_cf = confusion_matrix(test_y,y_pred_rf)\n",
    "print('RandomForest:')\n",
    "print(classification_report(test_y, y_pred_rf))\n",
    "f1rf = '{0:.4g}'.format(f1_score(test_y, y_pred_rf, average='weighted'))\n",
    "accrf = '{0:.4g}'.format(accuracy_score(test_y, y_pred_rf))\n",
    "print('RandomForest F1 Score: ',f1rf)\n",
    "print('RandomForest Accuracy Score: ',accrf)\n",
    "\n",
    "#Plot Confusion Metrixes\n",
    "f,axes = plt.subplots(ncols=3,figsize=(36,12))\n",
    "sns.heatmap(svc_cf, annot=True,fmt='2.0f',cmap='summer',ax=axes[0])\n",
    "axes[0].set_title('SupportVectorMachines \\n Confusion Matrix')\n",
    "\n",
    "sns.heatmap(knn_cf, annot=True,fmt='2.0f',cmap='summer',ax=axes[1])\n",
    "axes[1].set_title('KNearestNeighbors \\n Confusion Matrix')\n",
    "\n",
    "sns.heatmap(rf_cf, annot=True,fmt='2.0f',cmap='summer',ax=axes[2])\n",
    "axes[2].set_title('RandomForest \\n Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0bf147",
   "metadata": {},
   "source": [
    "### 4.1 Check Cross Validation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b82b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC\n",
    "#Define the pipeline\n",
    "step1 = list()\n",
    "step1.append(('scaler', MinMaxScaler()))\n",
    "step1.append(('model', SVC()))\n",
    "pipeline1 = Pipeline(steps=step1)\n",
    "#Define the evaluation procedure\n",
    "cv = StratifiedKFold(n_splits=5, random_state=42,shuffle=True)\n",
    "#Evaluate the model using cross-validation\n",
    "scores1 = cross_val_score(pipeline1, X[selfeat], y, scoring='f1_weighted', cv=cv, n_jobs=-1)\n",
    "#Report performance\n",
    "print('SVC Accuracy: %.3f (%.3f)' % (scores1.mean()*100, scores1.std()*100))\n",
    "\n",
    "#KNearestNeighbors\n",
    "#Define the pipeline\n",
    "step2 = list()\n",
    "step2.append(('scaler', MinMaxScaler()))\n",
    "step2.append(('model', KNeighborsClassifier()))\n",
    "pipeline2 = Pipeline(steps=step2)\n",
    "#Define the evaluation procedure\n",
    "cv = StratifiedKFold(n_splits=5, random_state=42,shuffle=True)\n",
    "#Evaluate the model using cross-validation\n",
    "scores2 = cross_val_score(pipeline2, X[selfeat], y, scoring='f1_weighted', cv=cv, n_jobs=-1)\n",
    "#Report performance\n",
    "print('KNearestNeighbors Accuracy: %.3f (%.3f)' % (scores2.mean()*100, scores2.std()*100))\n",
    "\n",
    "\n",
    "#RandomForest\n",
    "#Define the pipeline\n",
    "step3 = list()\n",
    "step3.append(('scaler', MinMaxScaler()))\n",
    "step3.append(('model', RandomForestClassifier()))\n",
    "pipeline3 = Pipeline(steps=step3)\n",
    "#Define the evaluation procedure\n",
    "cv = StratifiedKFold(n_splits=5, random_state=42,shuffle=True)\n",
    "#Evaluate the model using cross-validation\n",
    "scores3 = cross_val_score(pipeline3, X[selfeat], y, scoring='f1_weighted', cv=cv, n_jobs=-1)\n",
    "#Report performance\n",
    "print('RandomForest Accuracy: %.3f (%.3f)' % (scores3.mean()*100, scores3.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67c8e92",
   "metadata": {},
   "source": [
    "### 4.2 Tuning HyperParameters using GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e93b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SupportVectorMachines\n",
    "svc_params = {'C': [0.5, 0.6, 0.7,0.8, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n",
    "grid_svc = GridSearchCV(SVC(), svc_params, cv=5, scoring='f1_weighted', n_jobs= 4, verbose = 1)\n",
    "grid_svc.fit(train_x_norm[selfeat], train_y)\n",
    "svm_svc = grid_svc.best_params_\n",
    "print(svm_svc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217358ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "knears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "grid_knears = GridSearchCV(KNeighborsClassifier(), knears_params, cv=5, scoring='f1_weighted', n_jobs= 4, verbose = 1)\n",
    "grid_knears.fit(train_x_norm[selfeat], train_y)\n",
    "knears_neighbors = grid_knears.best_estimator_\n",
    "print(knears_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0bdea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForest\n",
    "rf_params = {'n_estimators': range(100,1000,100),'max_features': ['auto', 'sqrt'],'bootstrap': [True,False]}\n",
    "grid_rf = GridSearchCV(RandomForestClassifier(), rf_params, cv=5, scoring='f1_weighted', n_jobs= 4, verbose = 1)\n",
    "grid_rf.fit(train_x_norm[selfeat], train_y)\n",
    "rf_rf = grid_rf.best_estimator_\n",
    "print(rf_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe015bac",
   "metadata": {},
   "source": [
    "### 4.3 Apply HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7763b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SupportVectorMachines\n",
    "svmsvc = SVC(C=0.9, kernel='poly')\n",
    "svmsvc.fit(train_x_norm[selfeat],train_y)\n",
    "\n",
    "y_pred_svmsvc = svmsvc.predict(test_x_norm[selfeat])\n",
    "svmsvc_cf = confusion_matrix(test_y,y_pred_svmsvc)\n",
    "\n",
    "ax = sns.heatmap(svmsvc_cf, annot=True,fmt='2.0f',cmap='summer')\n",
    "ax.set_title('SupportVectorMachines_Tuned \\n Confusion Matrix')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('SupportVectorMachines_Tuned:')\n",
    "print(classification_report(test_y, y_pred_svmsvc))\n",
    "f1svmsvc = f1_score(test_y, y_pred_svmsvc, average='weighted')\n",
    "svmsvc_cv_score = cross_val_score(svmsvc, train_x_norm[selfeat], train_y, cv=10,scoring='f1_weighted')\n",
    "print(f'Cross Validation Score: {svmsvc_cv_score.mean():.4f}')\n",
    "print(f'F1 Score: {f1svmsvc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855241e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "knearneigh = KNeighborsClassifier(n_neighbors=3)\n",
    "knearneigh.fit(train_x_norm[selfeat],train_y)\n",
    "\n",
    "y_pred_knearneigh = knearneigh.predict(test_x_norm[selfeat])\n",
    "knearneigh_cf = confusion_matrix(test_y,y_pred_knearneigh)\n",
    "\n",
    "ax = sns.heatmap(knearneigh_cf, annot=True,fmt='2.0f',cmap='summer')\n",
    "ax.set_title('KNearestNeighbors_Tuned \\n Confusion Matrix')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('KNearestNeighbors_Tuned:')\n",
    "print(classification_report(test_y, y_pred_knearneigh))\n",
    "f1knearneigh = f1_score(test_y, y_pred_knearneigh, average='weighted')\n",
    "knearneigh_cv_score = cross_val_score(knearneigh, train_x_norm[selfeat], train_y, cv=10,scoring='f1_weighted')\n",
    "print(f'{knearneigh_cv_score.mean():.4f}')\n",
    "print(f'{f1knearneigh:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d86592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForest\n",
    "randomforest = RandomForestClassifier(bootstrap=False, max_features='sqrt', n_estimators=900)\n",
    "randomforest.fit(train_x_norm[selfeat],train_y)\n",
    "\n",
    "y_pred_randomforest = randomforest.predict(test_x_norm[selfeat])\n",
    "randomforest_cf = confusion_matrix(test_y,y_pred_randomforest)\n",
    "\n",
    "ax = sns.heatmap(randomforest_cf, annot=True,fmt='2.0f',cmap='summer')\n",
    "ax.set_title('RandomForest_Tuned \\n Confusion Matrix')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('RandomForest_Tuned:')\n",
    "print(classification_report(test_y, y_pred_randomforest))\n",
    "f1randomforest = f1_score(test_y, y_pred_randomforest, average='weighted')\n",
    "rf_cv_score = cross_val_score(randomforest, train_x_norm[selfeat], train_y, cv=10,scoring='f1_weighted')\n",
    "print(f'{rf_cv_score.mean():.4f}')\n",
    "print(f'{f1randomforest:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab73ecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Feature Importance\n",
    "print(\"Important features\")\n",
    "pd.Series(randomforest.feature_importances_,train_x_norm[selfeat].columns).sort_values(ascending=True).plot.barh(width=0.8)\n",
    "print('__'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468bc341",
   "metadata": {},
   "source": [
    "## Create Output for Submission\n",
    "### Model Selected: Support Vector Machines with tuning hyperparameters (C=0.9, kernel='poly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0369ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_norm['y'] = svmsvc.predict(df_test_norm[selfeat])\n",
    "df_test_norm.head()\n",
    "\n",
    "df_test_norm['y'].value_counts()\n",
    "ax = sns.countplot(x='y',data=df_test_norm, palette='hls')\n",
    "ax.bar_label(ax.containers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output\n",
    "predict = df_test_norm['y']\n",
    "fields = ['id','Expected']\n",
    "df = pd.DataFrame()\n",
    "id = list(range(1,2501))\n",
    "df['id']=pd.Series(id)\n",
    "df['Expected']=pd.Series(predict)\n",
    "df.to_csv('submit_svmsvc2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3710be6c",
   "metadata": {},
   "source": [
    "------------------------------------------ END -----------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
